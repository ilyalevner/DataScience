{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First stetps in Neural Network with Keras\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 53,
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# fix random seed for reproducibility\n",
<<<<<<< HEAD
    "np.random.seed(7)"
=======
    "np.random.seed(5)"
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =pd.read_csv()"
=======
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../data/mtcars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Mazda RX4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Mazda RX4 Wag</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Datsun 710</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Hornet 4 Drive</td>\n",
       "      <td>21.4</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.215</td>\n",
       "      <td>19.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Hornet Sportabout</td>\n",
       "      <td>18.7</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.440</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Valiant</td>\n",
       "      <td>18.1</td>\n",
       "      <td>6</td>\n",
       "      <td>225.0</td>\n",
       "      <td>105</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.460</td>\n",
       "      <td>20.22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Duster 360</td>\n",
       "      <td>14.3</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>245</td>\n",
       "      <td>3.21</td>\n",
       "      <td>3.570</td>\n",
       "      <td>15.84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Merc 240D</td>\n",
       "      <td>24.4</td>\n",
       "      <td>4</td>\n",
       "      <td>146.7</td>\n",
       "      <td>62</td>\n",
       "      <td>3.69</td>\n",
       "      <td>3.190</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Merc 230</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>140.8</td>\n",
       "      <td>95</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.150</td>\n",
       "      <td>22.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Merc 280</td>\n",
       "      <td>19.2</td>\n",
       "      <td>6</td>\n",
       "      <td>167.6</td>\n",
       "      <td>123</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.440</td>\n",
       "      <td>18.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Merc 280C</td>\n",
       "      <td>17.8</td>\n",
       "      <td>6</td>\n",
       "      <td>167.6</td>\n",
       "      <td>123</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.440</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Merc 450SE</td>\n",
       "      <td>16.4</td>\n",
       "      <td>8</td>\n",
       "      <td>275.8</td>\n",
       "      <td>180</td>\n",
       "      <td>3.07</td>\n",
       "      <td>4.070</td>\n",
       "      <td>17.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Merc 450SL</td>\n",
       "      <td>17.3</td>\n",
       "      <td>8</td>\n",
       "      <td>275.8</td>\n",
       "      <td>180</td>\n",
       "      <td>3.07</td>\n",
       "      <td>3.730</td>\n",
       "      <td>17.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Merc 450SLC</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8</td>\n",
       "      <td>275.8</td>\n",
       "      <td>180</td>\n",
       "      <td>3.07</td>\n",
       "      <td>3.780</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Cadillac Fleetwood</td>\n",
       "      <td>10.4</td>\n",
       "      <td>8</td>\n",
       "      <td>472.0</td>\n",
       "      <td>205</td>\n",
       "      <td>2.93</td>\n",
       "      <td>5.250</td>\n",
       "      <td>17.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Lincoln Continental</td>\n",
       "      <td>10.4</td>\n",
       "      <td>8</td>\n",
       "      <td>460.0</td>\n",
       "      <td>215</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.424</td>\n",
       "      <td>17.82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Chrysler Imperial</td>\n",
       "      <td>14.7</td>\n",
       "      <td>8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>230</td>\n",
       "      <td>3.23</td>\n",
       "      <td>5.345</td>\n",
       "      <td>17.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Fiat 128</td>\n",
       "      <td>32.4</td>\n",
       "      <td>4</td>\n",
       "      <td>78.7</td>\n",
       "      <td>66</td>\n",
       "      <td>4.08</td>\n",
       "      <td>2.200</td>\n",
       "      <td>19.47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Honda Civic</td>\n",
       "      <td>30.4</td>\n",
       "      <td>4</td>\n",
       "      <td>75.7</td>\n",
       "      <td>52</td>\n",
       "      <td>4.93</td>\n",
       "      <td>1.615</td>\n",
       "      <td>18.52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Toyota Corolla</td>\n",
       "      <td>33.9</td>\n",
       "      <td>4</td>\n",
       "      <td>71.1</td>\n",
       "      <td>65</td>\n",
       "      <td>4.22</td>\n",
       "      <td>1.835</td>\n",
       "      <td>19.90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Toyota Corona</td>\n",
       "      <td>21.5</td>\n",
       "      <td>4</td>\n",
       "      <td>120.1</td>\n",
       "      <td>97</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.465</td>\n",
       "      <td>20.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Dodge Challenger</td>\n",
       "      <td>15.5</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.520</td>\n",
       "      <td>16.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>AMC Javelin</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.435</td>\n",
       "      <td>17.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Camaro Z28</td>\n",
       "      <td>13.3</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>245</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.840</td>\n",
       "      <td>15.41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Pontiac Firebird</td>\n",
       "      <td>19.2</td>\n",
       "      <td>8</td>\n",
       "      <td>400.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.845</td>\n",
       "      <td>17.05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Fiat X1-9</td>\n",
       "      <td>27.3</td>\n",
       "      <td>4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>66</td>\n",
       "      <td>4.08</td>\n",
       "      <td>1.935</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Porsche 914-2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.3</td>\n",
       "      <td>91</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2.140</td>\n",
       "      <td>16.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>Lotus Europa</td>\n",
       "      <td>30.4</td>\n",
       "      <td>4</td>\n",
       "      <td>95.1</td>\n",
       "      <td>113</td>\n",
       "      <td>3.77</td>\n",
       "      <td>1.513</td>\n",
       "      <td>16.90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>Ford Pantera L</td>\n",
       "      <td>15.8</td>\n",
       "      <td>8</td>\n",
       "      <td>351.0</td>\n",
       "      <td>264</td>\n",
       "      <td>4.22</td>\n",
       "      <td>3.170</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>Ferrari Dino</td>\n",
       "      <td>19.7</td>\n",
       "      <td>6</td>\n",
       "      <td>145.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2.770</td>\n",
       "      <td>15.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>Maserati Bora</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>301.0</td>\n",
       "      <td>335</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.570</td>\n",
       "      <td>14.60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>Volvo 142E</td>\n",
       "      <td>21.4</td>\n",
       "      <td>4</td>\n",
       "      <td>121.0</td>\n",
       "      <td>109</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.780</td>\n",
       "      <td>18.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Unnamed: 0   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  \\\n",
       "0             Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1   \n",
       "1         Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1   \n",
       "2            Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1   \n",
       "3        Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0   \n",
       "4     Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0   \n",
       "5               Valiant  18.1    6  225.0  105  2.76  3.460  20.22   1   0   \n",
       "6            Duster 360  14.3    8  360.0  245  3.21  3.570  15.84   0   0   \n",
       "7             Merc 240D  24.4    4  146.7   62  3.69  3.190  20.00   1   0   \n",
       "8              Merc 230  22.8    4  140.8   95  3.92  3.150  22.90   1   0   \n",
       "9              Merc 280  19.2    6  167.6  123  3.92  3.440  18.30   1   0   \n",
       "10            Merc 280C  17.8    6  167.6  123  3.92  3.440  18.90   1   0   \n",
       "11           Merc 450SE  16.4    8  275.8  180  3.07  4.070  17.40   0   0   \n",
       "12           Merc 450SL  17.3    8  275.8  180  3.07  3.730  17.60   0   0   \n",
       "13          Merc 450SLC  15.2    8  275.8  180  3.07  3.780  18.00   0   0   \n",
       "14   Cadillac Fleetwood  10.4    8  472.0  205  2.93  5.250  17.98   0   0   \n",
       "15  Lincoln Continental  10.4    8  460.0  215  3.00  5.424  17.82   0   0   \n",
       "16    Chrysler Imperial  14.7    8  440.0  230  3.23  5.345  17.42   0   0   \n",
       "17             Fiat 128  32.4    4   78.7   66  4.08  2.200  19.47   1   1   \n",
       "18          Honda Civic  30.4    4   75.7   52  4.93  1.615  18.52   1   1   \n",
       "19       Toyota Corolla  33.9    4   71.1   65  4.22  1.835  19.90   1   1   \n",
       "20        Toyota Corona  21.5    4  120.1   97  3.70  2.465  20.01   1   0   \n",
       "21     Dodge Challenger  15.5    8  318.0  150  2.76  3.520  16.87   0   0   \n",
       "22          AMC Javelin  15.2    8  304.0  150  3.15  3.435  17.30   0   0   \n",
       "23           Camaro Z28  13.3    8  350.0  245  3.73  3.840  15.41   0   0   \n",
       "24     Pontiac Firebird  19.2    8  400.0  175  3.08  3.845  17.05   0   0   \n",
       "25            Fiat X1-9  27.3    4   79.0   66  4.08  1.935  18.90   1   1   \n",
       "26        Porsche 914-2  26.0    4  120.3   91  4.43  2.140  16.70   0   1   \n",
       "27         Lotus Europa  30.4    4   95.1  113  3.77  1.513  16.90   1   1   \n",
       "28       Ford Pantera L  15.8    8  351.0  264  4.22  3.170  14.50   0   1   \n",
       "29         Ferrari Dino  19.7    6  145.0  175  3.62  2.770  15.50   0   1   \n",
       "30        Maserati Bora  15.0    8  301.0  335  3.54  3.570  14.60   0   1   \n",
       "31           Volvo 142E  21.4    4  121.0  109  4.11  2.780  18.60   1   1   \n",
       "\n",
       "    gear  carb  \n",
       "0      4     4  \n",
       "1      4     4  \n",
       "2      4     1  \n",
       "3      3     1  \n",
       "4      3     2  \n",
       "5      3     1  \n",
       "6      3     4  \n",
       "7      4     2  \n",
       "8      4     2  \n",
       "9      4     4  \n",
       "10     4     4  \n",
       "11     3     3  \n",
       "12     3     3  \n",
       "13     3     3  \n",
       "14     3     4  \n",
       "15     3     4  \n",
       "16     3     4  \n",
       "17     4     1  \n",
       "18     4     2  \n",
       "19     4     1  \n",
       "20     3     1  \n",
       "21     3     2  \n",
       "22     3     2  \n",
       "23     3     4  \n",
       "24     3     2  \n",
       "25     4     1  \n",
       "26     5     2  \n",
       "27     5     2  \n",
       "28     5     4  \n",
       "29     5     6  \n",
       "30     5     8  \n",
       "31     4     2  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will use the mtcars dataset. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
=======
   "execution_count": 56,
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "nv = [\"mpg\",\"cyl\",\"disp\",\"hp\",\"drat\",\"wt\",\"qsec\",\"vs\",\"gear\",\"carb\"]\n",
    "X = np.array(dataset[nv])\n",
    "y = np.array(dataset[\"am\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Architecture\n",
    " \n",
    "First we have to import in keras the \"Sequential\" function. This is the most common NN type, that means we are going to define our NN one layer a time. The other possible architecture available is the \"Functional\" function, which permits a more flexible architecture by letting join many structures in parallel. \n",
    " \n",
    "Then we initialize the NN by defining our model object and will add to it the layers as we defined in our network architecture plan.\n",
    " \n",
    "The layers that we will use are fully conected layers, which means that we want that all the input data from the previous layer will be connedted with all the nodes of this layer. In keras we call this layer type \"Dense\".\n",
    "\n",
    "When defining the layer we have to provide some parameters: input_dim is the dimmention of the input (in our case we have 10 inputs (variables) that enter our hidden layer. \n",
    " \n",
    "Another parameter is the activation function. We can define this function inside the layer definition, or separately (see alternatives 1 and 2)."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 63,
=======
   "execution_count": 57,
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 65,
=======
   "execution_count": 58,
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(12, input_dim=10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 61,
=======
   "execution_count": 45,
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create the NN architecture\n",
    "#model = Sequential()\n",
    "#model.add(Dense(12, input_dim=10))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    " \n",
    "To run our NN we have first to \"compile\" our model. The compilation require us to define some important parameters:\n",
    " \n",
    "1. Define the <b>loss function</b>: this is the measure of the error. There are many types of loss functions that could be applied in NN's:\n",
    " - mean_squared_error\n",
    " - mean_absolute_error\n",
    " - mean_absolute_percentage_error\n",
    " - mean_squared_logarithmic_error\n",
    " - squared_hinge\n",
    " - hinge\n",
    " - categorical_hinge\n",
    " - logcosh\n",
    " - categorical_crossentropy\n",
    " - sparse_categorical_crossentropy\n",
    " - binary_crossentropy\n",
    " - kullback_leibler_divergence\n",
    " - poisson\n",
    " - cosine_proximity\n",
    " \n",
    " \n",
    "2. Define the <b>optimizer</b>: this is the method we want to apply for the gradient descent. Some popular methods are:\n",
    " - Stochastic gradient descent (SGD)\n",
    " - Adaptive moment estimator (Adam)\n",
    " - batch gradient descent\n",
    " - mini-batch gradient descent \n",
    " - Nesterov accelerated gradient (NAG)\n",
    " - Adagrad\n",
    " - AdaDelta\n",
    " - RMSprop\n",
    "\n",
    " \n",
    "3. Define the <b>metrics</b>: Metric values are recorded at the end of each epoch on the training dataset. The most common metrics are:\n",
    " + For regression output:\n",
    "    - Mean Squared Error (MSE)\n",
    "    - Mean Absolute Error (MAE)\n",
    "    - Mean Absolute Percentage Error (MAPE)\n",
    "    - Cosine Proximity \n",
    " \n",
    " + For classification output:\n",
    "    - Binary Accuracy\n",
    "    - Categorical Accuracy\n",
    "    - Sparse Categorical Accuracy\n",
    "    - Top k Categorical Accuracy\n",
    "    - Sparse Top k Categorical Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 66,
=======
   "execution_count": 59,
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compilation, the model could be trained. We use the fit function to begin the training. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 67,
=======
   "execution_count": 60,
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 1/150\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.2815 - acc: 0.7188\n",
      "Epoch 2/150\n",
      "32/32 [==============================] - 0s 291us/step - loss: 0.2812 - acc: 0.7187\n",
      "Epoch 3/150\n",
      "32/32 [==============================] - 0s 386us/step - loss: 0.2812 - acc: 0.7188\n",
      "Epoch 4/150\n",
      "32/32 [==============================] - 0s 267us/step - loss: 0.2812 - acc: 0.7187\n",
      "Epoch 5/150\n",
      "32/32 [==============================] - 0s 254us/step - loss: 0.2812 - acc: 0.7187\n",
      "Epoch 6/150\n",
      "32/32 [==============================] - 0s 255us/step - loss: 0.2811 - acc: 0.7188\n",
      "Epoch 7/150\n",
      "32/32 [==============================] - 0s 390us/step - loss: 0.2805 - acc: 0.7188\n",
      "Epoch 8/150\n",
      "32/32 [==============================] - 0s 390us/step - loss: 0.2684 - acc: 0.7187\n",
      "Epoch 9/150\n",
      "32/32 [==============================] - 0s 313us/step - loss: 0.1647 - acc: 0.8125\n",
      "Epoch 10/150\n",
      "32/32 [==============================] - 0s 309us/step - loss: 0.1082 - acc: 0.8438\n",
      "Epoch 11/150\n",
      "32/32 [==============================] - 0s 302us/step - loss: 0.1023 - acc: 0.8438\n",
      "Epoch 12/150\n",
      "32/32 [==============================] - 0s 347us/step - loss: 0.0988 - acc: 0.9062\n",
      "Epoch 13/150\n",
      "32/32 [==============================] - 0s 364us/step - loss: 0.1053 - acc: 0.8750\n",
      "Epoch 14/150\n",
      "32/32 [==============================] - 0s 346us/step - loss: 0.1048 - acc: 0.8750\n",
      "Epoch 15/150\n",
      "32/32 [==============================] - 0s 217us/step - loss: 0.1002 - acc: 0.9062\n",
      "Epoch 16/150\n",
      "32/32 [==============================] - 0s 250us/step - loss: 0.1015 - acc: 0.8750\n",
      "Epoch 17/150\n",
      "32/32 [==============================] - 0s 197us/step - loss: 0.0990 - acc: 0.8438\n",
      "Epoch 18/150\n",
      "32/32 [==============================] - 0s 222us/step - loss: 0.1038 - acc: 0.8437\n",
      "Epoch 19/150\n",
      "32/32 [==============================] - 0s 315us/step - loss: 0.0992 - acc: 0.9062\n",
      "Epoch 20/150\n",
      "32/32 [==============================] - 0s 258us/step - loss: 0.1255 - acc: 0.8750\n",
      "Epoch 21/150\n",
      "32/32 [==============================] - 0s 206us/step - loss: 0.1292 - acc: 0.8750\n",
      "Epoch 22/150\n",
      "32/32 [==============================] - 0s 476us/step - loss: 0.1158 - acc: 0.8750\n",
      "Epoch 23/150\n",
      "32/32 [==============================] - 0s 560us/step - loss: 0.0990 - acc: 0.8750\n",
      "Epoch 24/150\n",
      "32/32 [==============================] - 0s 277us/step - loss: 0.1000 - acc: 0.8438\n",
      "Epoch 25/150\n",
      "32/32 [==============================] - 0s 410us/step - loss: 0.0983 - acc: 0.8438\n",
      "Epoch 26/150\n",
      "32/32 [==============================] - 0s 383us/step - loss: 0.0969 - acc: 0.8750\n",
      "Epoch 27/150\n",
      "32/32 [==============================] - 0s 214us/step - loss: 0.0963 - acc: 0.8750\n",
      "Epoch 28/150\n",
      "32/32 [==============================] - 0s 310us/step - loss: 0.0957 - acc: 0.9062\n",
      "Epoch 29/150\n",
      "32/32 [==============================] - 0s 216us/step - loss: 0.0959 - acc: 0.9062\n",
      "Epoch 30/150\n",
      "32/32 [==============================] - 0s 236us/step - loss: 0.0959 - acc: 0.9062\n",
      "Epoch 31/150\n",
      "32/32 [==============================] - 0s 219us/step - loss: 0.0965 - acc: 0.9062\n",
      "Epoch 32/150\n",
      "32/32 [==============================] - 0s 252us/step - loss: 0.0950 - acc: 0.9062\n",
      "Epoch 33/150\n",
      "32/32 [==============================] - 0s 242us/step - loss: 0.0949 - acc: 0.9062\n",
      "Epoch 34/150\n",
      "32/32 [==============================] - 0s 238us/step - loss: 0.0952 - acc: 0.9062\n",
      "Epoch 35/150\n",
      "32/32 [==============================] - 0s 235us/step - loss: 0.0961 - acc: 0.9062\n",
      "Epoch 36/150\n",
      "32/32 [==============================] - 0s 294us/step - loss: 0.0940 - acc: 0.9062\n",
      "Epoch 37/150\n",
      "32/32 [==============================] - 0s 235us/step - loss: 0.0996 - acc: 0.9062\n",
      "Epoch 38/150\n",
      "32/32 [==============================] - 0s 197us/step - loss: 0.0976 - acc: 0.9063\n",
      "Epoch 39/150\n",
      "32/32 [==============================] - 0s 212us/step - loss: 0.0950 - acc: 0.9062\n",
      "Epoch 40/150\n",
      "32/32 [==============================] - 0s 370us/step - loss: 0.0938 - acc: 0.8750\n",
      "Epoch 41/150\n",
      "32/32 [==============================] - 0s 225us/step - loss: 0.0930 - acc: 0.8437\n",
      "Epoch 42/150\n",
      "32/32 [==============================] - 0s 237us/step - loss: 0.0999 - acc: 0.9062\n",
      "Epoch 43/150\n",
      "32/32 [==============================] - 0s 219us/step - loss: 0.1093 - acc: 0.8750\n",
      "Epoch 44/150\n",
      "32/32 [==============================] - 0s 286us/step - loss: 0.1115 - acc: 0.8750\n",
      "Epoch 45/150\n",
      "32/32 [==============================] - 0s 254us/step - loss: 0.0919 - acc: 0.9062\n",
      "Epoch 46/150\n",
      "32/32 [==============================] - 0s 227us/step - loss: 0.0917 - acc: 0.8438\n",
      "Epoch 47/150\n",
      "32/32 [==============================] - 0s 220us/step - loss: 0.0923 - acc: 0.8750\n",
      "Epoch 48/150\n",
      "32/32 [==============================] - 0s 201us/step - loss: 0.0929 - acc: 0.9062\n",
      "Epoch 49/150\n",
      "32/32 [==============================] - 0s 227us/step - loss: 0.0980 - acc: 0.9062\n",
      "Epoch 50/150\n",
      "32/32 [==============================] - 0s 206us/step - loss: 0.0976 - acc: 0.9062\n",
      "Epoch 51/150\n",
      "32/32 [==============================] - 0s 317us/step - loss: 0.0941 - acc: 0.9062\n",
      "Epoch 52/150\n",
      "32/32 [==============================] - 0s 227us/step - loss: 0.0925 - acc: 0.9062\n",
      "Epoch 53/150\n",
      "32/32 [==============================] - 0s 207us/step - loss: 0.0973 - acc: 0.9062\n",
      "Epoch 54/150\n",
      "32/32 [==============================] - 0s 201us/step - loss: 0.1003 - acc: 0.9062\n",
      "Epoch 55/150\n",
      "32/32 [==============================] - 0s 312us/step - loss: 0.0956 - acc: 0.9062\n",
      "Epoch 56/150\n",
      "32/32 [==============================] - 0s 231us/step - loss: 0.0922 - acc: 0.9062\n",
      "Epoch 57/150\n",
      "32/32 [==============================] - 0s 432us/step - loss: 0.0922 - acc: 0.9062\n",
      "Epoch 58/150\n",
      "32/32 [==============================] - 0s 201us/step - loss: 0.0915 - acc: 0.9062\n",
      "Epoch 59/150\n",
      "32/32 [==============================] - 0s 356us/step - loss: 0.0919 - acc: 0.8750\n",
      "Epoch 60/150\n",
      "32/32 [==============================] - 0s 248us/step - loss: 0.0922 - acc: 0.8438\n",
      "Epoch 61/150\n",
      "32/32 [==============================] - 0s 196us/step - loss: 0.0925 - acc: 0.8437\n",
      "Epoch 62/150\n",
      "32/32 [==============================] - 0s 205us/step - loss: 0.0915 - acc: 0.8437\n",
      "Epoch 63/150\n",
      "32/32 [==============================] - 0s 224us/step - loss: 0.0915 - acc: 0.8438\n",
      "Epoch 64/150\n",
      "32/32 [==============================] - 0s 207us/step - loss: 0.0913 - acc: 0.8438\n",
      "Epoch 65/150\n",
      "32/32 [==============================] - 0s 237us/step - loss: 0.0917 - acc: 0.8437\n",
      "Epoch 66/150\n",
      "32/32 [==============================] - 0s 209us/step - loss: 0.0907 - acc: 0.8750\n",
      "Epoch 67/150\n",
      "32/32 [==============================] - 0s 208us/step - loss: 0.0923 - acc: 0.9062\n",
      "Epoch 68/150\n",
      "32/32 [==============================] - 0s 314us/step - loss: 0.0960 - acc: 0.9062\n",
      "Epoch 69/150\n",
      "32/32 [==============================] - 0s 270us/step - loss: 0.0935 - acc: 0.9062\n",
      "Epoch 70/150\n",
      "32/32 [==============================] - 0s 230us/step - loss: 0.0920 - acc: 0.9062\n",
      "Epoch 71/150\n",
      "32/32 [==============================] - 0s 257us/step - loss: 0.0894 - acc: 0.9062\n",
      "Epoch 72/150\n",
      "32/32 [==============================] - 0s 255us/step - loss: 0.0915 - acc: 0.8437\n",
      "Epoch 73/150\n",
      "32/32 [==============================] - 0s 222us/step - loss: 0.0887 - acc: 0.8438\n",
      "Epoch 74/150\n",
      "32/32 [==============================] - 0s 211us/step - loss: 0.0893 - acc: 0.8437\n",
      "Epoch 75/150\n",
      "32/32 [==============================] - 0s 219us/step - loss: 0.0892 - acc: 0.8438\n",
      "Epoch 76/150\n",
      "32/32 [==============================] - 0s 269us/step - loss: 0.0874 - acc: 0.8438\n",
      "Epoch 77/150\n",
      "32/32 [==============================] - 0s 237us/step - loss: 0.0867 - acc: 0.8438\n",
      "Epoch 78/150\n",
      "32/32 [==============================] - 0s 370us/step - loss: 0.0875 - acc: 0.8437\n",
      "Epoch 79/150\n",
      "32/32 [==============================] - 0s 336us/step - loss: 0.0848 - acc: 0.8437\n",
      "Epoch 80/150\n",
      "32/32 [==============================] - 0s 168us/step - loss: 0.0852 - acc: 0.8750\n",
      "Epoch 81/150\n",
      "32/32 [==============================] - 0s 213us/step - loss: 0.0874 - acc: 0.9062\n",
      "Epoch 82/150\n",
      "32/32 [==============================] - 0s 226us/step - loss: 0.0961 - acc: 0.9062\n",
      "Epoch 83/150\n",
      "32/32 [==============================] - 0s 227us/step - loss: 0.0975 - acc: 0.9062\n",
      "Epoch 84/150\n",
      "32/32 [==============================] - 0s 236us/step - loss: 0.0926 - acc: 0.9062\n",
      "Epoch 85/150\n",
      "32/32 [==============================] - 0s 225us/step - loss: 0.0893 - acc: 0.9062\n",
      "Epoch 86/150\n",
      "32/32 [==============================] - 0s 250us/step - loss: 0.0875 - acc: 0.9062\n",
      "Epoch 87/150\n",
      "32/32 [==============================] - 0s 233us/step - loss: 0.0872 - acc: 0.9062\n",
      "Epoch 88/150\n",
      "32/32 [==============================] - 0s 249us/step - loss: 0.0881 - acc: 0.9062\n",
      "Epoch 89/150\n",
      "32/32 [==============================] - 0s 213us/step - loss: 0.0886 - acc: 0.9062\n",
      "Epoch 90/150\n",
      "32/32 [==============================] - 0s 225us/step - loss: 0.0887 - acc: 0.9062\n",
      "Epoch 91/150\n",
      "32/32 [==============================] - 0s 366us/step - loss: 0.0906 - acc: 0.8750\n",
      "Epoch 92/150\n",
      "32/32 [==============================] - 0s 316us/step - loss: 0.0947 - acc: 0.8437\n",
      "Epoch 93/150\n",
      "32/32 [==============================] - 0s 422us/step - loss: 0.0946 - acc: 0.8438\n",
      "Epoch 94/150\n",
      "32/32 [==============================] - 0s 350us/step - loss: 0.0832 - acc: 0.8438\n",
      "Epoch 95/150\n",
      "32/32 [==============================] - 0s 334us/step - loss: 0.0832 - acc: 0.8438\n",
      "Epoch 96/150\n",
      "32/32 [==============================] - 0s 266us/step - loss: 0.0837 - acc: 0.8437\n",
      "Epoch 97/150\n",
      "32/32 [==============================] - 0s 388us/step - loss: 0.0818 - acc: 0.8438\n",
      "Epoch 98/150\n",
      "32/32 [==============================] - 0s 365us/step - loss: 0.0821 - acc: 0.8438\n",
      "Epoch 99/150\n",
      "32/32 [==============================] - 0s 247us/step - loss: 0.0816 - acc: 0.8437\n",
      "Epoch 100/150\n",
      "32/32 [==============================] - 0s 363us/step - loss: 0.0804 - acc: 0.8437\n",
      "Epoch 101/150\n",
      "32/32 [==============================] - 0s 209us/step - loss: 0.0819 - acc: 0.8438\n",
      "Epoch 102/150\n",
      "32/32 [==============================] - 0s 345us/step - loss: 0.0878 - acc: 0.8750\n",
      "Epoch 103/150\n",
      "32/32 [==============================] - 0s 253us/step - loss: 0.0783 - acc: 0.8750\n",
      "Epoch 104/150\n",
      "32/32 [==============================] - 0s 314us/step - loss: 0.0794 - acc: 0.8437\n",
      "Epoch 105/150\n",
      "32/32 [==============================] - 0s 242us/step - loss: 0.0840 - acc: 0.8750\n",
      "Epoch 106/150\n",
      "32/32 [==============================] - 0s 233us/step - loss: 0.0775 - acc: 0.8437\n",
      "Epoch 107/150\n",
      "32/32 [==============================] - 0s 399us/step - loss: 0.0765 - acc: 0.8750\n",
      "Epoch 108/150\n",
      "32/32 [==============================] - 0s 330us/step - loss: 0.0813 - acc: 0.9062\n",
      "Epoch 109/150\n",
      "32/32 [==============================] - 0s 370us/step - loss: 0.0835 - acc: 0.9062\n",
      "Epoch 110/150\n",
      "32/32 [==============================] - 0s 224us/step - loss: 0.0805 - acc: 0.9062\n",
      "Epoch 111/150\n",
      "32/32 [==============================] - 0s 225us/step - loss: 0.0799 - acc: 0.9062\n",
      "Epoch 112/150\n",
      "32/32 [==============================] - 0s 283us/step - loss: 0.0778 - acc: 0.9062\n",
      "Epoch 113/150\n",
      "32/32 [==============================] - 0s 240us/step - loss: 0.0832 - acc: 0.8437\n",
      "Epoch 114/150\n",
      "32/32 [==============================] - 0s 252us/step - loss: 0.0810 - acc: 0.8750\n",
      "Epoch 115/150\n",
      "32/32 [==============================] - 0s 245us/step - loss: 0.0790 - acc: 0.8437\n",
      "Epoch 116/150\n",
      "32/32 [==============================] - 0s 205us/step - loss: 0.0836 - acc: 0.9062\n",
      "Epoch 117/150\n",
      "32/32 [==============================] - 0s 324us/step - loss: 0.0868 - acc: 0.9062\n",
      "Epoch 118/150\n",
      "32/32 [==============================] - 0s 260us/step - loss: 0.0859 - acc: 0.9062\n",
      "Epoch 119/150\n",
      "32/32 [==============================] - 0s 346us/step - loss: 0.0807 - acc: 0.9062\n",
      "Epoch 120/150\n",
      "32/32 [==============================] - 0s 225us/step - loss: 0.0774 - acc: 0.9062\n",
      "Epoch 121/150\n",
      "32/32 [==============================] - 0s 245us/step - loss: 0.0742 - acc: 0.8750\n",
      "Epoch 122/150\n",
      "32/32 [==============================] - 0s 332us/step - loss: 0.0775 - acc: 0.8438\n",
      "Epoch 123/150\n",
      "32/32 [==============================] - 0s 210us/step - loss: 0.0796 - acc: 0.9062\n",
      "Epoch 124/150\n",
      "32/32 [==============================] - 0s 243us/step - loss: 0.0840 - acc: 0.9062\n",
      "Epoch 125/150\n",
      "32/32 [==============================] - 0s 261us/step - loss: 0.0844 - acc: 0.9062\n",
      "Epoch 126/150\n",
      "32/32 [==============================] - 0s 221us/step - loss: 0.0849 - acc: 0.9062\n",
      "Epoch 127/150\n",
      "32/32 [==============================] - 0s 191us/step - loss: 0.0788 - acc: 0.9062\n",
      "Epoch 128/150\n",
      "32/32 [==============================] - 0s 259us/step - loss: 0.0774 - acc: 0.9062\n",
      "Epoch 129/150\n",
      "32/32 [==============================] - 0s 317us/step - loss: 0.0775 - acc: 0.9062\n",
      "Epoch 130/150\n",
      "32/32 [==============================] - 0s 279us/step - loss: 0.0776 - acc: 0.9062\n",
      "Epoch 131/150\n",
      "32/32 [==============================] - 0s 423us/step - loss: 0.0797 - acc: 0.8437\n",
      "Epoch 132/150\n",
      "32/32 [==============================] - 0s 254us/step - loss: 0.0700 - acc: 0.8750\n",
      "Epoch 133/150\n",
      "32/32 [==============================] - 0s 368us/step - loss: 0.0771 - acc: 0.9062\n",
      "Epoch 134/150\n",
      "32/32 [==============================] - 0s 277us/step - loss: 0.0896 - acc: 0.9062\n",
      "Epoch 135/150\n",
      "32/32 [==============================] - 0s 364us/step - loss: 0.0914 - acc: 0.9062\n",
      "Epoch 136/150\n",
      "32/32 [==============================] - 0s 267us/step - loss: 0.0855 - acc: 0.9062\n",
      "Epoch 137/150\n",
      "32/32 [==============================] - 0s 262us/step - loss: 0.0783 - acc: 0.9063\n",
      "Epoch 138/150\n",
      "32/32 [==============================] - 0s 413us/step - loss: 0.0813 - acc: 0.8438\n",
      "Epoch 139/150\n",
      "32/32 [==============================] - 0s 305us/step - loss: 0.0750 - acc: 0.9062\n",
      "Epoch 140/150\n",
      "32/32 [==============================] - 0s 352us/step - loss: 0.0734 - acc: 0.9062\n",
      "Epoch 141/150\n",
      "32/32 [==============================] - 0s 278us/step - loss: 0.0706 - acc: 0.8750\n",
      "Epoch 142/150\n",
      "32/32 [==============================] - 0s 407us/step - loss: 0.0745 - acc: 0.9062\n",
      "Epoch 143/150\n",
      "32/32 [==============================] - 0s 258us/step - loss: 0.0796 - acc: 0.9062\n",
      "Epoch 144/150\n",
      "32/32 [==============================] - 0s 390us/step - loss: 0.0812 - acc: 0.9062\n",
      "Epoch 145/150\n",
      "32/32 [==============================] - 0s 299us/step - loss: 0.0809 - acc: 0.9062\n",
      "Epoch 146/150\n",
      "32/32 [==============================] - 0s 423us/step - loss: 0.0801 - acc: 0.9062\n",
      "Epoch 147/150\n",
      "32/32 [==============================] - 0s 243us/step - loss: 0.0748 - acc: 0.9062\n",
      "Epoch 148/150\n",
      "32/32 [==============================] - 0s 381us/step - loss: 0.0708 - acc: 0.8750\n",
      "Epoch 149/150\n",
      "32/32 [==============================] - 0s 282us/step - loss: 0.0728 - acc: 0.8750\n",
      "Epoch 150/150\n",
      "32/32 [==============================] - 0s 329us/step - loss: 0.0698 - acc: 0.9062\n"
=======
      "Epoch 1/180\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 2/180\n",
      "32/32 [==============================] - 0s 119us/step - loss: 0.4063 - acc: 0.5937\n",
      "Epoch 3/180\n",
      "32/32 [==============================] - 0s 107us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 4/180\n",
      "32/32 [==============================] - 0s 109us/step - loss: 0.4063 - acc: 0.5937\n",
      "Epoch 5/180\n",
      "32/32 [==============================] - 0s 112us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 6/180\n",
      "32/32 [==============================] - 0s 100us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 7/180\n",
      "32/32 [==============================] - 0s 104us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 8/180\n",
      "32/32 [==============================] - 0s 95us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 9/180\n",
      "32/32 [==============================] - 0s 97us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 10/180\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 11/180\n",
      "32/32 [==============================] - 0s 93us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 12/180\n",
      "32/32 [==============================] - 0s 99us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 13/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 14/180\n",
      "32/32 [==============================] - 0s 103us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 15/180\n",
      "32/32 [==============================] - 0s 95us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 16/180\n",
      "32/32 [==============================] - 0s 95us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 17/180\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 18/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 19/180\n",
      "32/32 [==============================] - 0s 98us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 20/180\n",
      "32/32 [==============================] - 0s 101us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 21/180\n",
      "32/32 [==============================] - 0s 98us/step - loss: 0.4063 - acc: 0.5937\n",
      "Epoch 22/180\n",
      "32/32 [==============================] - 0s 110us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 23/180\n",
      "32/32 [==============================] - 0s 106us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 24/180\n",
      "32/32 [==============================] - 0s 101us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 25/180\n",
      "32/32 [==============================] - 0s 116us/step - loss: 0.4063 - acc: 0.5937\n",
      "Epoch 26/180\n",
      "32/32 [==============================] - 0s 92us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 27/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 28/180\n",
      "32/32 [==============================] - 0s 106us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 29/180\n",
      "32/32 [==============================] - 0s 112us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 30/180\n",
      "32/32 [==============================] - 0s 99us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 31/180\n",
      "32/32 [==============================] - 0s 103us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 32/180\n",
      "32/32 [==============================] - 0s 104us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 33/180\n",
      "32/32 [==============================] - 0s 100us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 34/180\n",
      "32/32 [==============================] - 0s 102us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 35/180\n",
      "32/32 [==============================] - 0s 102us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 36/180\n",
      "32/32 [==============================] - 0s 100us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 37/180\n",
      "32/32 [==============================] - 0s 97us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 38/180\n",
      "32/32 [==============================] - 0s 103us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 39/180\n",
      "32/32 [==============================] - 0s 99us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 40/180\n",
      "32/32 [==============================] - 0s 101us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 41/180\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 42/180\n",
      "32/32 [==============================] - 0s 103us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 43/180\n",
      "32/32 [==============================] - 0s 107us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 44/180\n",
      "32/32 [==============================] - 0s 100us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 45/180\n",
      "32/32 [==============================] - 0s 104us/step - loss: 0.4063 - acc: 0.5937\n",
      "Epoch 46/180\n",
      "32/32 [==============================] - 0s 109us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 47/180\n",
      "32/32 [==============================] - 0s 111us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 48/180\n",
      "32/32 [==============================] - 0s 119us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 49/180\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 50/180\n",
      "32/32 [==============================] - 0s 104us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 51/180\n",
      "32/32 [==============================] - 0s 92us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 52/180\n",
      "32/32 [==============================] - 0s 94us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 53/180\n",
      "32/32 [==============================] - 0s 104us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 54/180\n",
      "32/32 [==============================] - 0s 88us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 55/180\n",
      "32/32 [==============================] - 0s 101us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 56/180\n",
      "32/32 [==============================] - 0s 110us/step - loss: 0.4063 - acc: 0.5938\n",
      "Epoch 57/180\n",
      "32/32 [==============================] - 0s 100us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 58/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 59/180\n",
      "32/32 [==============================] - 0s 97us/step - loss: 0.4063 - acc: 0.5938\n",
      "Epoch 60/180\n",
      "32/32 [==============================] - 0s 92us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 61/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 62/180\n",
      "32/32 [==============================] - 0s 99us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 63/180\n",
      "32/32 [==============================] - 0s 92us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 64/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 65/180\n",
      "32/32 [==============================] - 0s 100us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 66/180\n",
      "32/32 [==============================] - 0s 85us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 67/180\n",
      "32/32 [==============================] - 0s 103us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 68/180\n",
      "32/32 [==============================] - 0s 88us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 69/180\n",
      "32/32 [==============================] - 0s 87us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 70/180\n",
      "32/32 [==============================] - 0s 102us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 71/180\n",
      "32/32 [==============================] - 0s 86us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 72/180\n",
      "32/32 [==============================] - 0s 101us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 73/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 74/180\n",
      "32/32 [==============================] - 0s 87us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 75/180\n",
      "32/32 [==============================] - 0s 85us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 76/180\n",
      "32/32 [==============================] - 0s 86us/step - loss: 0.4063 - acc: 0.5937\n",
      "Epoch 77/180\n",
      "32/32 [==============================] - 0s 87us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 78/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 79/180\n",
      "32/32 [==============================] - 0s 86us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 80/180\n",
      "32/32 [==============================] - 0s 87us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 81/180\n",
      "32/32 [==============================] - 0s 86us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 82/180\n",
      "32/32 [==============================] - 0s 82us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 83/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 84/180\n",
      "32/32 [==============================] - 0s 95us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 85/180\n",
      "32/32 [==============================] - 0s 96us/step - loss: 0.4063 - acc: 0.5937\n",
      "Epoch 86/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 89us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 87/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 88/180\n",
      "32/32 [==============================] - 0s 98us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 89/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 90/180\n",
      "32/32 [==============================] - 0s 88us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 91/180\n",
      "32/32 [==============================] - 0s 85us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 92/180\n",
      "32/32 [==============================] - 0s 113us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 93/180\n",
      "32/32 [==============================] - 0s 103us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 94/180\n",
      "32/32 [==============================] - 0s 102us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 95/180\n",
      "32/32 [==============================] - 0s 95us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 96/180\n",
      "32/32 [==============================] - 0s 95us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 97/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 98/180\n",
      "32/32 [==============================] - 0s 105us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 99/180\n",
      "32/32 [==============================] - 0s 86us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 100/180\n",
      "32/32 [==============================] - 0s 88us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 101/180\n",
      "32/32 [==============================] - 0s 86us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 102/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 103/180\n",
      "32/32 [==============================] - 0s 86us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 104/180\n",
      "32/32 [==============================] - 0s 106us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 105/180\n",
      "32/32 [==============================] - 0s 81us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 106/180\n",
      "32/32 [==============================] - 0s 81us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 107/180\n",
      "32/32 [==============================] - 0s 85us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 108/180\n",
      "32/32 [==============================] - 0s 88us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 109/180\n",
      "32/32 [==============================] - 0s 88us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 110/180\n",
      "32/32 [==============================] - 0s 84us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 111/180\n",
      "32/32 [==============================] - 0s 86us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 112/180\n",
      "32/32 [==============================] - 0s 88us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 113/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 114/180\n",
      "32/32 [==============================] - 0s 81us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 115/180\n",
      "32/32 [==============================] - 0s 83us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 116/180\n",
      "32/32 [==============================] - 0s 82us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 117/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 118/180\n",
      "32/32 [==============================] - 0s 84us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 119/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 120/180\n",
      "32/32 [==============================] - 0s 87us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 121/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 122/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4063 - acc: 0.5937\n",
      "Epoch 123/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4063 - acc: 0.5938\n",
      "Epoch 124/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4063 - acc: 0.5937\n",
      "Epoch 125/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 126/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 127/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 128/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 129/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 130/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 131/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 132/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 133/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 134/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 135/180\n",
      "32/32 [==============================] - 0s 89us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 136/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 137/180\n",
      "32/32 [==============================] - 0s 82us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 138/180\n",
      "32/32 [==============================] - 0s 86us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 139/180\n",
      "32/32 [==============================] - 0s 98us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 140/180\n",
      "32/32 [==============================] - 0s 92us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 141/180\n",
      "32/32 [==============================] - 0s 101us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 142/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 143/180\n",
      "32/32 [==============================] - 0s 101us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 144/180\n",
      "32/32 [==============================] - 0s 79us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 145/180\n",
      "32/32 [==============================] - 0s 81us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 146/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 147/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 148/180\n",
      "32/32 [==============================] - 0s 82us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 149/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 150/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 151/180\n",
      "32/32 [==============================] - 0s 80us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 152/180\n",
      "32/32 [==============================] - 0s 81us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 153/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 154/180\n",
      "32/32 [==============================] - 0s 90us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 155/180\n",
      "32/32 [==============================] - 0s 92us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 156/180\n",
      "32/32 [==============================] - 0s 82us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 157/180\n",
      "32/32 [==============================] - 0s 92us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 158/180\n",
      "32/32 [==============================] - 0s 106us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 159/180\n",
      "32/32 [==============================] - 0s 82us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 160/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 161/180\n",
      "32/32 [==============================] - 0s 92us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 162/180\n",
      "32/32 [==============================] - 0s 82us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 163/180\n",
      "32/32 [==============================] - 0s 83us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 164/180\n",
      "32/32 [==============================] - 0s 104us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 165/180\n",
      "32/32 [==============================] - 0s 81us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 166/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 167/180\n",
      "32/32 [==============================] - 0s 82us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 168/180\n",
      "32/32 [==============================] - 0s 82us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 169/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 170/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 84us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 171/180\n",
      "32/32 [==============================] - 0s 84us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 172/180\n",
      "32/32 [==============================] - 0s 84us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 173/180\n",
      "32/32 [==============================] - 0s 104us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 174/180\n",
      "32/32 [==============================] - 0s 81us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 175/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 176/180\n",
      "32/32 [==============================] - 0s 105us/step - loss: 0.4062 - acc: 0.5937\n",
      "Epoch 177/180\n",
      "32/32 [==============================] - 0s 83us/step - loss: 0.4063 - acc: 0.5937\n",
      "Epoch 178/180\n",
      "32/32 [==============================] - 0s 84us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 179/180\n",
      "32/32 [==============================] - 0s 83us/step - loss: 0.4062 - acc: 0.5938\n",
      "Epoch 180/180\n",
      "32/32 [==============================] - 0s 91us/step - loss: 0.4062 - acc: 0.5937\n"
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
<<<<<<< HEAD
    "res = model.fit(X, y, epochs=150, batch_size=10)"
=======
    "res = model.fit(X, y, epochs=180, batch_size=20)"
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 48,
=======
   "execution_count": 61,
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "[0.40624956227838993,\n",
       " 0.4062491562217474,\n",
       " 0.40624842792749405,\n",
       " 0.4062456227838993,\n",
       " 0.4062412353232503,\n",
       " 0.4062330015003681,\n",
       " 0.4062241595238447,\n",
       " 0.4061837336048484,\n",
       " 0.40611765906214714,\n",
       " 0.40596948377788067,\n",
       " 0.40542138926684856,\n",
       " 0.4037764435636964,\n",
       " 0.39911687560379505,\n",
       " 0.38090869411826134,\n",
       " 0.3362861927598715,\n",
       " 0.3108447901904583,\n",
       " 0.299159524962306,\n",
       " 0.26942321844398975,\n",
       " 0.26680739875882864,\n",
       " 0.25051996554248035,\n",
       " 0.22875933116301894,\n",
       " 0.20914421137422323,\n",
       " 0.20248003024607897,\n",
       " 0.17830559937283397,\n",
       " 0.16673500714387046,\n",
       " 0.16437597310869023,\n",
       " 0.1581480170643772,\n",
       " 0.1471378414425999,\n",
       " 0.13746317033655941,\n",
       " 0.13216423685662448,\n",
       " 0.16219257260672748,\n",
       " 0.18526306096464396,\n",
       " 0.14460759103531018,\n",
       " 0.1324675614014268,\n",
       " 0.13310734368860722,\n",
       " 0.13076360616832972,\n",
       " 0.13151038251817226,\n",
       " 0.12694758037105203,\n",
       " 0.12941963749472052,\n",
       " 0.12515254132449627,\n",
       " 0.12428966886363924,\n",
       " 0.12476699007675052,\n",
       " 0.1205764461774379,\n",
       " 0.11986408499069512,\n",
       " 0.12187551683746278,\n",
       " 0.12633424415253103,\n",
       " 0.12564866547472775,\n",
       " 0.1208040143828839,\n",
       " 0.1211845395155251,\n",
       " 0.11895984958391637,\n",
       " 0.11766338525922038,\n",
       " 0.11774341471027583,\n",
       " 0.11731415777467191,\n",
       " 0.11697324772831053,\n",
       " 0.11691910261288285,\n",
       " 0.11730923620052636,\n",
       " 0.11595211818348616,\n",
       " 0.11774565931409597,\n",
       " 0.11994543857872486,\n",
       " 0.11352075915783644,\n",
       " 0.12439136113971472,\n",
       " 0.1298210491804639,\n",
       " 0.1248104739934206,\n",
       " 0.11864680331200361,\n",
       " 0.11910778842866421,\n",
       " 0.10922805761219934,\n",
       " 0.11400805413722992,\n",
       " 0.11326594420825131,\n",
       " 0.1129974900883548,\n",
       " 0.11309275376390815,\n",
       " 0.11113456258863152,\n",
       " 0.11053550755605102,\n",
       " 0.11033076629973948,\n",
       " 0.11051285720895976,\n",
       " 0.11469720979221165,\n",
       " 0.11357512068934739,\n",
       " 0.1105213847476989,\n",
       " 0.11036882712505758,\n",
       " 0.10789895371999592,\n",
       " 0.10788632254116237,\n",
       " 0.10941341299621854,\n",
       " 0.1071832834277302,\n",
       " 0.11201513686683029,\n",
       " 0.11098201852291822,\n",
       " 0.1120072603225708,\n",
       " 0.11429749010130763,\n",
       " 0.11389927915297449,\n",
       " 0.10879510127415415,\n",
       " 0.10525945760309696,\n",
       " 0.1072464850731194,\n",
       " 0.12152171600610018,\n",
       " 0.11065102159045637,\n",
       " 0.10631743409248884,\n",
       " 0.10969230614050574,\n",
       " 0.10940346377901733,\n",
       " 0.10987634802586399,\n",
       " 0.1075185620575212,\n",
       " 0.10693158367939759,\n",
       " 0.10625657175478409,\n",
       " 0.10477994731627405,\n",
       " 0.1081995204440318,\n",
       " 0.12635906995274127,\n",
       " 0.11465889308601618,\n",
       " 0.0986405797302723,\n",
       " 0.1061901992361527,\n",
       " 0.11591431591659784,\n",
       " 0.11323257824551547,\n",
       " 0.10876749630551785,\n",
       " 0.10465437942184508,\n",
       " 0.10168386378791183,\n",
       " 0.10661743860691786,\n",
       " 0.10432798159308732,\n",
       " 0.10509995723259635,\n",
       " 0.1009531244635582,\n",
       " 0.1133039987180382,\n",
       " 0.11172861616839924,\n",
       " 0.1102083281148225,\n",
       " 0.11177456192672253,\n",
       " 0.1032621004851535,\n",
       " 0.11094413491082378,\n",
       " 0.10753633547574282,\n",
       " 0.10685178032144904,\n",
       " 0.10586033726576716,\n",
       " 0.10494705733435694,\n",
       " 0.1015434917062521,\n",
       " 0.0944723675493151,\n",
       " 0.10316711629275233,\n",
       " 0.11834928998723626,\n",
       " 0.10493614431470633,\n",
       " 0.09378134063445032,\n",
       " 0.10391516707022674,\n",
       " 0.1031118003265874,\n",
       " 0.10183231194969267,\n",
       " 0.09866115031763911,\n",
       " 0.09600913233589381,\n",
       " 0.10223542631138116,\n",
       " 0.09794376732315868,\n",
       " 0.09710149657530565,\n",
       " 0.09459439106285572,\n",
       " 0.0914518988574855,\n",
       " 0.09393043047748506,\n",
       " 0.0938186775892973,\n",
       " 0.09341250907164067,\n",
       " 0.09329241997329518,\n",
       " 0.09405884365514794,\n",
       " 0.09475176851265132,\n",
       " 0.09479686990380287,\n",
       " 0.09713891567662358,\n",
       " 0.09837770942326252,\n",
       " 0.0991839140187949]"
      ]
     },
     "execution_count": 48,
=======
       "[0.4062499962747097,\n",
       " 0.40625000931322575,\n",
       " 0.40625,\n",
       " 0.40625000931322575,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625000931322575,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625000931322575,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625000931322575,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062500074505806,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625001583248377,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625000931322575,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625000931322575,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625000931322575,\n",
       " 0.40625000186264515,\n",
       " 0.40625000931322575,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625000931322575,\n",
       " 0.40625,\n",
       " 0.4062499962747097,\n",
       " 0.40625]"
      ]
     },
     "execution_count": 61,
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will check the performance of the model by comparing the prediction (yhat) with the original label (y)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 68,
=======
   "execution_count": 63,
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "32/32 [==============================] - 0s 2ms/step\n",
      "\n",
      "acc: 90.62%\n"
=======
      "32/32 [==============================] - 0s 35us/step\n",
      "\n",
      "acc: 59.38%\n"
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using the model\n",
    "\n",
    "We can then use the predict function to run the trained model on a new (test) dataset:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 69,
=======
   "execution_count": 64,
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "predictions = model.predict(X)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "#print(rounded)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 70,
=======
   "execution_count": 65,
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "    y  yhat\n",
      "0   1   0.0\n",
      "1   1   0.0\n",
      "2   1   1.0\n",
      "3   0   0.0\n",
      "4   0   0.0\n",
      "5   0   0.0\n",
      "6   0   0.0\n",
      "7   0   0.0\n",
      "8   0   0.0\n",
      "9   0   0.0\n",
      "10  0   0.0\n",
      "11  0   0.0\n",
      "12  0   0.0\n",
      "13  0   0.0\n",
      "14  0   0.0\n",
      "15  0   0.0\n",
      "16  0   0.0\n",
      "17  1   1.0\n",
      "18  1   1.0\n",
      "19  1   1.0\n",
      "20  0   1.0\n",
      "21  0   0.0\n",
      "22  0   0.0\n",
      "23  0   0.0\n",
      "24  0   0.0\n",
      "25  1   1.0\n",
      "26  1   1.0\n",
      "27  1   1.0\n",
      "28  1   1.0\n",
      "29  1   1.0\n",
      "30  1   1.0\n",
      "31  1   1.0\n"
=======
      "    yhat  y\n",
      "0    0.0  1\n",
      "1    0.0  1\n",
      "2    0.0  1\n",
      "3    0.0  0\n",
      "4    0.0  0\n",
      "5    0.0  0\n",
      "6    0.0  0\n",
      "7    0.0  0\n",
      "8    0.0  0\n",
      "9    0.0  0\n",
      "10   0.0  0\n",
      "11   0.0  0\n",
      "12   0.0  0\n",
      "13   0.0  0\n",
      "14   0.0  0\n",
      "15   0.0  0\n",
      "16   0.0  0\n",
      "17   0.0  1\n",
      "18   0.0  1\n",
      "19   0.0  1\n",
      "20   0.0  0\n",
      "21   0.0  0\n",
      "22   0.0  0\n",
      "23   0.0  0\n",
      "24   0.0  0\n",
      "25   0.0  1\n",
      "26   0.0  1\n",
      "27   0.0  1\n",
      "28   0.0  1\n",
      "29   0.0  1\n",
      "30   0.0  1\n",
      "31   0.0  1\n"
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame({'yhat':rounded,'y':y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"/assets/conv-demo/index.html\" width=\"100%\" height=\"700px;\" style=\"border:none;\"></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.3"
=======
   "version": "3.6.8"
>>>>>>> 75213b409d63192386c03e5155c099fa6970f665
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
